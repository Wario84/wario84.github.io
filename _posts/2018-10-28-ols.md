---
title: "Derivation of Ordinary Least Squares"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


\section{Deriving Ordinary Least Squares (OLS)}
Is an estimation method for linear relationships between one or more explanatory variable(s), or independent variables $x_i$, and a response variable variable $y$ on a $\beta_p$ set of parameter(s) . A key assumption about the estimation is that there is an underlying model or structural equation that if correctly specified, it will contain all the explanatory variables systematically affecting the response variable. The interest of the estimation lies on finding values for the  $\beta_p$ unknown parameters that measure the partial effect of an independent variable(s) on the response variable.

\begin{equation}
y_i= \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots \beta_px_p
\end{equation}

This structural model is a system of equations with $\textit{p}$ unknown coefficients for each explanatory variable and one intercept and $\textit{i}$ equations. In order to solve the system, or in other words, to find estimates for the parameters, is necessary to consider three scenarios:

\begin{itemize}
 \item $i<p$ The system is underidentified and inconsistent with infinite of solutions. 
 \item $i=p$ The system is identified, consistent with unique solutions.
 \item $i>p$ The system is overidentified, usually inconsistent with multiple solutions.
\end{itemize}

OLS typically is a method that attempts to find solutions for the overidentified system. For instance, in real life applications, there is a certain population process that is at least partially explained by our structural model. Additionally, we have data that contains information or observations about this process, where usually we have more equations (observations) than unknown parameters in our model. The goal is to find a unique set of coefficients for each \textit{p} explanatory variable(s) that best fit certain criteria. For instance, the model $y_i=\beta_1x_1 + \beta_2x_2$ has no intercept and two unknown parameters which can take values of any real number, for simplicity let set $\beta_1=4$ and $\beta_2=3$.

\begin{align*}
24 =  (4)*2 + (3)*4 + \epsilon \\
18 = (4)*3 + (3)*2 + \epsilon \\
19 = (4)*4 + (3)*1 + \epsilon \\
\end{align*}

The difference $\epsilon$, between the response variable $y$ and the estimated value is called \textit{residual term}. A criteria to find estimates for the parameters is to find a set $\beta_p$ that minimizes in some way the residuals, for instance, we can think on the average residuals $\bar{\epsilon}$. The problem with this criteria is that $\epsilon$ can take positive and negative values, and the arithmetic mean possible will fail to capture the difference between the predicted value and the response variable. A more suitable approach is perhaps to think of $\epsilon$ as distances, in such a case the goal is to find a set $\beta_p$ that minimizes the \textit{sum of the squared residuals}.

\begin{equation}
\epsilon =  (y_i - \beta_0 - \beta_1x_1 - \beta_2x_2 - \dots \beta_px_p)^2
\end{equation}

```{ols-1}
n <- 200

i <- rep(1, n)
x <- rnorm(n,4,1)
g <- rnorm(n,0,1)
z <- rnorm(n,3,1)


### Coefficients
b<- c(.8, 4, 1.5, 5)

X <- cbind(i, x, g, z)

### Response Varible
y = X%*%b + rnorm(nrow(X),0,1)

```
